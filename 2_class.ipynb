{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the raw data :')\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import re\n",
    "import random\n",
    "\n",
    "# some hyperparameter type things :)\n",
    "sr = 200\n",
    "t = 200\n",
    "ch = 19\n",
    "affix = '_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from .pkl files (\"raw\")\n",
    "raw_features = {}\n",
    "raw_labels = {}\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: \n",
    "    raw_features[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_features{affix}.npy')\n",
    "    raw_labels[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_labels{affix}.npy').reshape(-1)\n",
    "    fileList.append((f'subj_{letter}', raw_features[f'subj_{letter}'], raw_labels[f'subj_{letter}']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 1 ... 4 2 4]\n",
      "[0 0 1 ... 0 0 0]\n",
      "[0 1 3 ... 2 4 4]\n",
      "[0 1 0 ... 0 0 0]\n",
      "[4 0 0 ... 2 4 4]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 3 1 ... 4 2 4]\n",
      "[0 0 1 ... 0 0 0]\n",
      "[3 1 4 ... 2 4 4]\n",
      "[0 1 0 ... 0 0 0]\n",
      "[0 3 1 ... 2 4 4]\n",
      "[0 0 1 ... 0 0 0]\n",
      "[0 3 4 0 4 2 3 3 4 3 3 3 1 0 3 4 2 2 3 1 3 0 4 2 3 2 4 2 0 4 4 4 0 3 1 2 4\n",
      " 3 1 0 2 4 2 0 0 1 0 1 3 2 1 0 2 0 0 0 4 0 2 0 0 4 2 4 1 2 1 0 2 4 2 1 1 3\n",
      " 1 0 4 2 1 4 0 3 3 2 3 3 1 3 4 2 2 2 4 3 4 1 3 1 2 4 1 1 1 2 4 2 2 3 1 1 2\n",
      " 0 4 4 2 1 4 2 2 3 1 0 4 0 1 0 2 3 4 3 0 4 3 4 0 2 2 0 2 4 3 3 4 4 2 0 0 4\n",
      " 0 2 3 1 0 0 4 1 2 1 3 0 0 4 0 2 0 3 0 2 2 1 1 3 3 2 4 4 2 2 0 2 3 3 0 2 2\n",
      " 3 2 0 0 0 2 2 3 1 1 3 1 3 1 3 3 4 2 1 2 2 1 0 1 0 0 2 3 3 1 1 3 3 4 2 4 1\n",
      " 4 2 1 0 2 1 3 4 0 3 1 3 1 2 0 1 3 3 3 2 1 1 4 4 3 4 1 3 1 2 3 4 2 2 0 3 3\n",
      " 2 0 3 0 0 4 4 2 4 0 3 2 4 2 2 3 1 0 3 4 3 4 0 4 4 2 0 1 2 0 0 0 1 3 2 0 4\n",
      " 1 2 2 4 0 0 2 4 3 4 4 0 0 4 1 2 0 2 1 0 3 4 4 3 1 2 4 0 0 1 1 3 4 3 2 2 0\n",
      " 4 1 2 4 0 2 4 1 1 2 3 0 4 0 1 3 0 3 3 3 3 1 3 3 0 2 4 3 0 3 3 3 0 1 3 3 3\n",
      " 4 1 3 3 1 3 1 1 2 3 1 2 2 0 1 4 3 2 0 0 3 3 1 3 2 0 3 0 0 2 3 0 0 0 0 1 1\n",
      " 4 2 1 4 2 0 4 1 0 2 2 0 0 4 3 2 2 1 2 2 2 0 1 3 4 1 3 3 3 1 4 0 4 3 4 3 4\n",
      " 0 1 1 0 0 3 3 2 3 0 3 3 1 1 3 2 4 0 3 4 3 1 0 0 1 3 0 1 1 0 0 3 3 3 4 3 1\n",
      " 2 1 2 4 0 0 4 4 3 2 4 0 0 0 4 2 4 4 1 1 3 2 3 2 2 3 1 1 2 1 1 3 0 4 0 3 1\n",
      " 3 3 0 3 0 0 0 0 2 1 1 2 3 1 2 4 2 0 4 3 1 0 3 1 4 2 2 3 3 2 4 4 4 1 1 3 2\n",
      " 0 2 3 1 0 0 1 2 4 2 0 0 2 1 4 0 3 2 1 1 1 4 4 0 3 2 4 1 0 3 2 4 1 4 4 2 1\n",
      " 2 1 2 3 3 0 1 4 4 2 4 3 0 3 1 4 1 4 3 3 2 4 2 2 4 1 1 4 2 0 2 4 4 3 0 1 1\n",
      " 4 2 4 2 2 4 0 2 0 1 1 2 1 2 4 2 0 0 1 0 0 3 4 4 2 0 0 4 3 4 0 4 2 4 1 2 3\n",
      " 3 2 0 3 4 1 1 3 3 3 0 2 0 4 4 3 1 0 2 0 0 4 2 0 0 0 0 4 4 1 2 3 3 0 2 4 2\n",
      " 2 3 1 4 0 1 4 1 0 0 1 2 2 1 0 0 0 0 0 2 4 2 2 0 0 4 4 0 3 2 3 1 4 1 4 2 4\n",
      " 4 2 0 0 2 0 1 1 4 4 0 0 1 3 2 2 3 3 0 1 0 3 3 0 1 4 0 4 0 2 3 3 2 0 1 4 0\n",
      " 4 1 1 0 0 0 3 2 0 3 4 4 1 1 3 0 1 4 3 2 2 0 2 3 2 4 1 3 0 2 3 1 4 1 4 4 2\n",
      " 4 2 4 1 0 2 0 4 0 4 1 1 3 1 4 2 1 3 0 0 4 3 4 0 4 2 0 3 0 2 4 3 4 1 3 3 4\n",
      " 4 0 2 4 2 3 4 2 1 0 0 1 2 0 2 3 4 2 4 1 4 1 4 2 0 0 0 3 0 2 0 1 2 0 0 1 4\n",
      " 2 1 3 2 4 3 2 1 3 1 4 0 0 0 4 1 2 0 4 3 1 3 3 0 1 0 3 4 4 3 4 2 4 4]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "[0 3 1 ... 2 4 4]\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# separate into \"thumb vs all\" and \"index vs all\" ...\n",
    "for subj, features, labels in fileList:\n",
    "    thumb_labels = np.where(labels == 1, 1, 0)\n",
    "    index_labels = np.where(labels == 2, 1, 0)\n",
    "    middle_labels = np.where(labels == 3, 1, 0)\n",
    "    ring_labels = np.where(labels == 4, 1, 0)\n",
    "    pinky_labels = np.where(labels == 5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D CNN w/ Adham's recommendations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ConvNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet_2D, self).__init__()\n",
    "        ### FILL IN ### [10 POINTS]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, (7, 3)),\n",
    "            nn.ReLU(), \n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, (5, 3)),\n",
    "            nn.Conv2d(16, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(121728, 256),\n",
    "            nn.Linear(21056, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4), # all features norefer\n",
    "            # nn.Linear(1408, 256), #pseudosampled featrues .\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        x = self.hidden(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "accuracies = []\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = fft_features[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    conv_net = ConvNet_2D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    for epoch in range(50):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = X_train\n",
    "        targets = y_train\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "\n",
    "    # Evaluate the neural network\n",
    "    conv_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = X_test\n",
    "        targets = y_test\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj}= \",correct/total)\n",
    "    accuracies.append(correct/total)\n",
    "\n",
    "print(f'avg accuracy = {np.mean(accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
