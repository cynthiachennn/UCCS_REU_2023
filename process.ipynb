{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# some hyperparameter type things :)\n",
    "sr = 200\n",
    "t = 200\n",
    "ch = 19\n",
    "affix = '_19_noref'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: obtain epochs and evokeds representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "/tmp/ipykernel_1207795/2458603808.py:4: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1826 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "3811 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "1899 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "2848 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "2874 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "1901 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "922 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "1917 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n",
      "/tmp/ipykernel_1207795/2458603808.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  file = mne.concatenate_epochs(subjects)\n"
     ]
    }
   ],
   "source": [
    "# read info from all files.\n",
    "subjList = ['A_405', 'A_408', 'B_110', 'B_309', 'B_311', 'B_316', 'C_204', 'C_429', 'E_321', 'E_415', 'E_429', 'F_027', 'F_209', 'F_210', 'G_413', 'G_428', 'H_804', 'I_719', 'I_723'] # exclude 'G_413' because it only has 3 events? and the other G only has like 30 events so like wtf :(  \n",
    "fileNameList = [(subj, f'preprocessing/{subj}{affix}.set') for subj in subjList]\n",
    "fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
    "fileList = [(subj, file, (mne.events_from_annotations(file, verbose=False))) for subj, file in fileList]\n",
    "fileList = [(subj, mne.Epochs(file, events[0], event_id=[1, 2, 3, 4, 5],tmin= 0, tmax=t/sr, baseline=None, preload=True, verbose=False)) for subj, file, events in fileList]\n",
    "\n",
    "fileListDict = dict(fileList)\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: # \n",
    "    r = re.compile(f'{letter}_d*')\n",
    "    subjects = list(filter(r.match, subjList))\n",
    "    subjects = [fileListDict[x] for x in subjects]\n",
    "    fileName = f'subj_{letter}'\n",
    "    file = mne.concatenate_epochs(subjects)\n",
    "    fileList.append((f'subj_{letter}', file, file.events[:, -1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evokeds = {subject[1][c].average() for c in ['1', '2', '3', '4', '5']}\n",
    "for c in evokeds:\n",
    "    c.plot_joint(title=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data as features/labels\n",
    "all_features = {}\n",
    "all_labels = {}\n",
    "\n",
    "finalList = []\n",
    "for subj, file, events in fileList:\n",
    "    features = file.get_data()\n",
    "    features = features[:, :, :-1]\n",
    "    labels = file.events[:, -1] - 1\n",
    "    finalList.append((subj, features, labels))\n",
    "    np.save(f'pickles/{subj}_features{affix}', features)\n",
    "    np.save(f'pickles/{subj}_labels{affix}', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from .pkl files (\"raw\")\n",
    "all_features = {}\n",
    "all_labels = {}\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: \n",
    "    all_features[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_features{affix}.npy')\n",
    "    all_labels[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_labels{affix}.npy')\n",
    "    fileList.append((f'subj_{letter}', all_features[f'subj_{letter}'], all_labels[f'subj_{letter}']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudosampling\n",
    "\n",
    "window = 100 # window size\n",
    "strides = [75, 50, 25, 25, 50, 75, 0] # step size (but actually it's better to make it BELL CURVE OMGGGGGGG (for overlap % maybe then...))\n",
    "pseudosampled_features = {}\n",
    "pseudosampled_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    features = pd.DataFrame(columns=[j for i in range(19) for j in range(100)])\n",
    "    for trial_i in range(file.events.shape[0]):\n",
    "        start = 0\n",
    "        for s in strides:\n",
    "            end = start + window\n",
    "            features.loc[len(features)] = file.get_data(item=trial_i)[0].T[start:end].reshape(19*window)\n",
    "            start += s\n",
    "    pseudosampled_features[subj] = features\n",
    "    pseudosampled_labels[subj] = labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: FFT Time !! slay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do fft for everyone :)\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_freqs = fftpack.fftfreq(t) # since all samples are the same, sample freqs are the same\n",
    "\n",
    "fft_features = {}\n",
    "fft_labels = {}\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    # file.event_id = {'thumb':1, 'index':2, 'middle':3, 'ring':4, 'pinkie':5}\n",
    "    # conditions = ['thumb']\n",
    "    fft_features[subj] = np.zeros((labels.shape[0], ch, int(t/2)))\n",
    "    for trial_i in range(labels.shape[0]): #for each event\n",
    "        fft = np.zeros((ch, int(t/2)))\n",
    "        for channel_i in range(ch):\n",
    "            sample = features[trial_i][channel_i]\n",
    "            sig_fft = fftpack.fft(sample)\n",
    "            power = np.abs(sig_fft)\n",
    "            sample_freq = fftpack.fftfreq(sample.size)\n",
    "            fft[channel_i] = power[0:int(power.size/2)]\n",
    "        fft_features[subj][trial_i] = fft\n",
    "    fft_labels[subj] = labels\n",
    "    features = fft_features[subj]\n",
    "#   print(f'{subj}: {file.events.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_stft(file, trial, channel):\n",
    "    #sample = file.get_data(item=trial, picks=channel)[0][0]\n",
    "    sample = file.loc[trial][channel*400:(channel+1)*400]\n",
    "    f, t, Zxx = stft(sample, fs=200, window='triang', nperseg=100, return_onesided=True, boundary= None)\n",
    "    Zxx = np.abs(Zxx)\n",
    "    print(Zxx.shape)\n",
    "    # plt.pcolormesh([t], f, np.abs(Zxx), vmin=Zxx.min(), vmax=Zxx.max(), shading='gourand')\n",
    "    # plt.title('STFT Magnitude')\n",
    "    # plt.ylabel('Frequency [Hz]')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.show()\n",
    "    return f, t, Zxx\n",
    "\n",
    "file = all_features['subj_A']\n",
    "STFT = get_stft(file, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = fileList[0:1]\n",
    "STFT_features = {}\n",
    "STFT_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    features = pd.DataFrame(columns=[j for j in range(387) for i in range(19)])\n",
    "    for trial_i in range(labels.shape[0]): #for each event\n",
    "        features_flat = []\n",
    "        for channel_i in range(19): #srry hardcoding this cuz lazy\n",
    "            sample = file.loc[trial_i][channel_i*400:(channel_i+1)*400]\n",
    "            f, t, Zxx = stft(sample, fs=200, window='triang', boundary= None)\n",
    "            features_flat.extend(np.reshape(Zxx, (Zxx.shape[0]*Zxx.shape[1])))\n",
    "        features.loc[len(features)] = features_flat\n",
    "    # features.to_csv(f'{subj}/fft/features.csv', index=False)\n",
    "    # labels.to_csv(f'{subj}/fft/labels.csv', index=False)\n",
    "    STFT_features[subj] = features\n",
    "    STFT_labels[subj] = labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ConvNet_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet_1D, self).__init__()\n",
    "        ### FILL IN ### [10 POINTS]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(19, 32, 3),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(121728, 256),\n",
    "            nn.Linear(6080, 256), # all features norefer\n",
    "            # nn.Linear(1408, 256), #pseudosampled featrues .\n",
    "            nn.Linear(256, 5)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STFT_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m subj, features, labels \u001b[39min\u001b[39;00m fileList:\n\u001b[0;32m----> 9\u001b[0m     features \u001b[39m=\u001b[39m STFT_features[subj]\n\u001b[1;32m     10\u001b[0m     labels \u001b[39m=\u001b[39m STFT_labels[subj]\n\u001b[1;32m     11\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(features, labels, test_size\u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m) \u001b[39m# 70% training 30% test\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STFT_features' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = STFT_features[subj]\n",
    "    labels = STFT_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    conv_net = ConvNet_1D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    for epoch in range(10):\n",
    "        print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = X_train.to_numpy()\n",
    "        targets = y_train.to_numpy()\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 19, int(data.shape[1]/19)), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "\n",
    "        # Evaluate the neural network\n",
    "        conv_net.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            data = X_test.to_numpy()\n",
    "            targets = y_test.to_numpy()\n",
    "            data = torch.tensor(data.reshape(int(data.shape[0]), 19, int(data.shape[1]/19)), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "            data = data.to(device)\n",
    "            targets = torch.tensor(targets, dtype=torch.int64)\n",
    "            targets = targets.to(device)\n",
    "            conv_net_predictions = conv_net(data)\n",
    "            _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        print(\"test accuracy = \",correct/total)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eeg(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idc):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ConvNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet_2D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, (7, 3)),\n",
    "            nn.ReLU(), \n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, (5, 3)),\n",
    "            nn.Conv2d(16, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(121728, 256),\n",
    "            nn.Linear(21056, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4), # all features norefer\n",
    "            # nn.Linear(1408, 256), #pseudosampled featrues .\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# binary class ?:D\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m subj, features, labels \u001b[39min\u001b[39;00m fileList[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m     joint \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39;49massign(label\u001b[39m=\u001b[39mlabels\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m      4\u001b[0m     joint \u001b[39m=\u001b[39m  joint\u001b[39m.\u001b[39mloc[joint[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin([\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m])]\n\u001b[1;32m      5\u001b[0m     features \u001b[39m=\u001b[39m joint\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "# binary class ?:D\n",
    "for subj, features, labels in fileList[0:1]:\n",
    "    joint = features.assign(label=labels.values)\n",
    "    joint =  joint.loc[joint['label'].isin([0, 4])]\n",
    "    features = joint.drop(['label'], axis=1)\n",
    "    labels = joint['label']\n",
    "\n",
    "    all_features[subj] = features\n",
    "    all_labels[subj] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      7\u001b[0m \u001b[39m# train neural net\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mset_device(\u001b[39m3\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m~/cynthia/lib/python3.10/site-packages/torch/cuda/__init__.py:350\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    348\u001b[0m device \u001b[39m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 350\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_setDevice(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# run CNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "torch.cuda.set_device(3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# device = 'cpu'\n",
    "accuracies = []\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = fft_features[subj]\n",
    "    labels = fft_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.2, random_state=42) # 70% training 30% test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= 0.2, random_state=42) # 70% training 30% test\n",
    "    conv_net = ConvNet_2D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    train_acc = 0\n",
    "    for epoch in range(50):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = X_train\n",
    "        targets = y_train\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        total, correct = 0, 0\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "        train_acc = correct/total\n",
    "\n",
    "    print(f\"train accuracy {subj}= \", train_acc) \n",
    "\n",
    "    # Evaluate the neural network\n",
    "    conv_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = X_test\n",
    "        targets = y_test\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj}= \",correct/total, '****')\n",
    "    accuracies.append(correct/total)\n",
    "\n",
    "print(f'avg accuracy = {np.mean(accuracies)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN w/ channels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 19, 200) (1826,)\n",
      "(1826, 19, 100) (1826, 19, 100)\n"
     ]
    }
   ],
   "source": [
    "print(fileList[0][1].shape, fileList[0][2].shape)\n",
    "print(fft_features['subj_A'].shape, fft_features['subj_A'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 1900)\n",
      "Accuracy subj_A: 0.2937956204379562\n",
      "(3811, 1900)\n",
      "Accuracy subj_B: 0.3723776223776224\n",
      "(1899, 1900)\n",
      "Accuracy subj_C: 0.41578947368421054\n",
      "(2848, 1900)\n",
      "Accuracy subj_E: 0.3157894736842105\n",
      "(2874, 1900)\n",
      "Accuracy subj_F: 0.25028968713789107\n",
      "(1901, 1900)\n",
      "Accuracy subj_G: 0.31523642732049034\n",
      "(922, 1900)\n",
      "Accuracy subj_H: 0.21299638989169675\n",
      "(1917, 1900)\n",
      "Accuracy subj_I: 0.2881944444444444\n"
     ]
    }
   ],
   "source": [
    "# SVM TIME WOOO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = fft_features[subj].reshape(fft_features[subj].shape[0], -1)\n",
    "    # print(features.shape)\n",
    "    labels = fft_labels[subj]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    \n",
    "    classifier = svm.SVC(kernel='rbf') # radial basis kernel; gamma should be 0.1\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TIME WOOO\n",
    "# SVM w/ 2 seconds of data takes much more time and does not improve accuracy :(\n",
    "# w/ 0.85 seconds it works .2 % maybe better and takes 1/3 of the time\n",
    "\n",
    "avg_acc = 0\n",
    "for subj, features, labels in fileList:\n",
    "    features = STFT_features[subj]\n",
    "    labels = STFT_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    classifier = svm.SVC(kernel='rbf') # radial basis kernel; gamma should be 0.1\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    avg_acc += metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Average accuracy: \", avg_acc/len(fileList))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest ? !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for subj, file in fileList:\n",
    "    features = pseudosampled_features[subj]\n",
    "    labels = pseudosampled_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: try morlet??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfr.data.shape)\n",
    "freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequencies of interest\n",
    "freqs = np.logspace(*np.log10([6, 30]), num=20) \n",
    "\n",
    "for subj, file in fileList:\n",
    "    tfr = mne.time_frequency.tfr_morlet(file, freqs=freqs, n_cycles=freqs / 2., return_itc=False)\n",
    "    features = tfr.data.reshape((tfr.data.shape[0]*tfr.data.shape[1]* tfr.data.shape[2]))\n",
    "    labels = [file.events[trial][2] for trial in file.events.shape[0]]  # Extract labels from the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CSP features :D\n",
    "# use raw data frames not mne.epochs datastructure\n",
    "\n",
    "from mne.decoding import CSP\n",
    "\n",
    "csp_features = {}\n",
    "csp_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    csp = CSP(n_components=5, reg=None, log=True)\n",
    "    data = np.reshape(all_features[subj], (all_features[subj].shape[0], 19, 401))\n",
    "    labels = all_labels[subj]\n",
    "\n",
    "    csp_features[subj] = csp.fit_transform(data, labels)\n",
    "    csp_labels[subj] = labels\n",
    "\n",
    "    csp.plot_filters(file.info, title='CSP patterns for %s' % subj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
