{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: obtain epochs and evokeds representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:5: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1826 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "3811 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1899 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "2848 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "2874 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1901 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "922 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "1917 matching events found\n",
      "No baseline correction applied\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9400\\3877480089.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  fileList.append((fileName, mne.concatenate_epochs(subjects)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('subj_A',\n",
       "  <Epochs |  1826 events (all good), 0 - 2 sec, baseline off, ~106.2 MB, data loaded,\n",
       "   '1': 405\n",
       "   '2': 333\n",
       "   '3': 377\n",
       "   '4': 360\n",
       "   '5': 351>),\n",
       " ('subj_B',\n",
       "  <Epochs |  3811 events (all good), 0 - 2 sec, baseline off, ~221.6 MB, data loaded,\n",
       "   '1': 844\n",
       "   '2': 708\n",
       "   '3': 774\n",
       "   '4': 741\n",
       "   '5': 744>),\n",
       " ('subj_C',\n",
       "  <Epochs |  1899 events (all good), 0 - 2 sec, baseline off, ~110.4 MB, data loaded,\n",
       "   '1': 371\n",
       "   '2': 372\n",
       "   '3': 389\n",
       "   '4': 380\n",
       "   '5': 387>),\n",
       " ('subj_E',\n",
       "  <Epochs |  2848 events (all good), 0 - 2 sec, baseline off, ~165.6 MB, data loaded,\n",
       "   '1': 620\n",
       "   '2': 512\n",
       "   '3': 590\n",
       "   '4': 567\n",
       "   '5': 559>),\n",
       " ('subj_F',\n",
       "  <Epochs |  2874 events (all good), 0 - 2 sec, baseline off, ~167.1 MB, data loaded,\n",
       "   '1': 622\n",
       "   '2': 519\n",
       "   '3': 594\n",
       "   '4': 573\n",
       "   '5': 566>),\n",
       " ('subj_G',\n",
       "  <Epochs |  1901 events (all good), 0 - 2 sec, baseline off, ~110.5 MB, data loaded,\n",
       "   '1': 415\n",
       "   '2': 342\n",
       "   '3': 395\n",
       "   '4': 375\n",
       "   '5': 374>),\n",
       " ('subj_H',\n",
       "  <Epochs |  922 events (all good), 0 - 2 sec, baseline off, ~53.6 MB, data loaded,\n",
       "   '1': 203\n",
       "   '2': 167\n",
       "   '3': 186\n",
       "   '4': 184\n",
       "   '5': 182>),\n",
       " ('subj_I',\n",
       "  <Epochs |  1917 events (all good), 0 - 2 sec, baseline off, ~111.5 MB, data loaded,\n",
       "   '1': 417\n",
       "   '2': 345\n",
       "   '3': 396\n",
       "   '4': 381\n",
       "   '5': 378>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read info from all files.\n",
    "subjList = ['A_405', 'A_408', 'B_110', 'B_309', 'B_311', 'B_316', 'C_204', 'C_429', 'E_321', 'E_415', 'E_429', 'F_027', 'F_209', 'F_210', 'G_413', 'G_428', 'H_804', 'I_719', 'I_723'] # exclude 'G_413' because it only has 3 events? and the other G only has like 30 events so like wtf :(  \n",
    "conditions = ['thumb', 'index', 'middle', 'ring', 'pinkie']\n",
    "fileNameList = [(subj, f'{subj}/{subj}_PREP.set') for subj in subjList]\n",
    "fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
    "fileList = [(subj, file, (mne.events_from_annotations(file, verbose=False))) for subj, file in fileList]\n",
    "fileList = [(subj, mne.Epochs(file, events[0], event_id=[1, 2, 3, 4, 5],tmin= 0, tmax=2, baseline=None, preload=True, verbose=False)) for subj, file, events in fileList]\n",
    "\n",
    "fileListDict = dict(fileList)\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: # \n",
    "    r = re.compile(f'{letter}_d*')\n",
    "    subjects = list(filter(r.match, subjList))\n",
    "    subjects = [fileListDict[x] for x in subjects]\n",
    "    fileName = f'subj_{letter}'\n",
    "    fileList.append((fileName, mne.concatenate_epochs(subjects)))\n",
    "\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = fileList[2]\n",
    "subject[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject[1]['2'] # look at specific event details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evokeds = {subject[1][c].average() for c in ['1', '2', '3', '4', '5']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in evokeds:\n",
    "    c.plot_joint(title=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = fileList[0][1]\n",
    "file.get_data(item=0).shape\n",
    "file.get_data(item=0).reshape(19*401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data as features/labels\n",
    "all_features = {}\n",
    "all_labels = {}\n",
    "\n",
    "for subj, file in fileList:\n",
    "    labels = pd.Series()\n",
    "    features = pd.DataFrame(columns=[j for i in range(19) for j in range(401)])\n",
    "    for trial_i in range(file.events.shape[0]):\n",
    "        labels.loc[len(labels)] = file.events[trial_i][2]\n",
    "        features.loc[len(features)] = file.get_data(item=trial_i).reshape(19*401) # :p i loveee hardcoding :D\n",
    "    all_features[subj] = features\n",
    "    all_labels[subj] = labels\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 19)\n",
      "(19, 401)\n"
     ]
    }
   ],
   "source": [
    "print(fileList[0][1].get_data(item=trial_i)[0].T[start:end].shape)\n",
    "print(fileList[0][1].get_data(item=2)[0][:][0:100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m         labels\u001b[39m.\u001b[39mloc[\u001b[39mlen\u001b[39m(labels)] \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mevents[trial_i][\u001b[39m2\u001b[39m]\n\u001b[0;32m     15\u001b[0m         end \u001b[39m=\u001b[39m start \u001b[39m+\u001b[39m window\n\u001b[1;32m---> 16\u001b[0m         features\u001b[39m.\u001b[39;49mloc[\u001b[39mlen\u001b[39;49m(features)] \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mget_data(item\u001b[39m=\u001b[39mtrial_i)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mT[start:end]\u001b[39m.\u001b[39mreshape(\u001b[39m19\u001b[39m\u001b[39m*\u001b[39mwindow)\n\u001b[0;32m     17\u001b[0m         start \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m s\n\u001b[0;32m     18\u001b[0m pseudosampled_features[subj] \u001b[39m=\u001b[39m features\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 849\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1825\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1822\u001b[0m     indexer, missing \u001b[39m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1824\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m-> 1825\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_missing(indexer, value)\n\u001b[0;32m   1826\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mloc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1829\u001b[0m     \u001b[39m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2180\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_mgr\n\u001b[0;32m   2179\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_append(value)\u001b[39m.\u001b[39m_mgr\n\u001b[0;32m   2181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_maybe_update_cacher(clear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9558\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   9555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   9556\u001b[0m     to_concat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m, other]\n\u001b[1;32m-> 9558\u001b[0m result \u001b[39m=\u001b[39m concat(\n\u001b[0;32m   9559\u001b[0m     to_concat,\n\u001b[0;32m   9560\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m   9561\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m   9562\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   9563\u001b[0m )\n\u001b[0;32m   9564\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[1;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Owner\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py:232\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m vals \u001b[39m=\u001b[39m [ju\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units]\n\u001b[0;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m blk\u001b[39m.\u001b[39mis_extension:\n\u001b[0;32m    229\u001b[0m     \u001b[39m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[39m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[39m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(vals, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[39m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     values \u001b[39m=\u001b[39m concat_compat(vals, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pseudosampling\n",
    "\n",
    "window = 100 # window size\n",
    "strides = [75, 50, 25, 25, 50, 75, 0] # step size (but actually it's better to make it BELL CURVE OMGGGGGGG (for overlap % maybe then...))\n",
    "pseudosampled_features = {}\n",
    "pseudosampled_labels = {}\n",
    "\n",
    "for subj, file in fileList:\n",
    "    labels = pd.Series()\n",
    "    features = pd.DataFrame(columns=[j for i in range(19) for j in range(100)])\n",
    "    for trial_i in range(file.events.shape[0]):\n",
    "        start = 0\n",
    "        for s in strides:\n",
    "            labels.loc[len(labels)] = file.events[trial_i][2]\n",
    "            end = start + window\n",
    "            print(start, end)\n",
    "            features.loc[len(features)] = file.get_data(item=trial_i)[0].T[start:end].reshape(19*window)\n",
    "            start += s\n",
    "    pseudosampled_features[subj] = features\n",
    "    pseudosampled_labels[subj] = labels\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: FFT Time !! slay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_fft(file, trial, channel): # pull one piece of that data set, trial 1 of thumb data\n",
    "    sample = file.get_data(item=trial, picks=channel)[0][0]\n",
    "    #fig1 = plt.plot(np.linspace(0, 1.5, num=300), sample)\n",
    "    # fft on that one piece...\n",
    "\n",
    "    sig_fft = fftpack.fft(sample)\n",
    "    power = np.abs(sig_fft)**2\n",
    "    sample_freq = fftpack.fftfreq(sample.size)\n",
    "    return power[0:int(power.size/2)]\n",
    "    # fig2 = plt.plot(sample_freq[0:200], power[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do fft for everyone :)\n",
    "# but acutally only try w one pereson frist ;D\n",
    "\n",
    "sr = 200 # sample rate\n",
    "t = 400 # sample size\n",
    "sample_freqs = fftpack.fftfreq(t) # since all samples are the same, sample freqs are the same\n",
    "\n",
    "all_features = {}\n",
    "all_labels = {}\n",
    "\n",
    "for subj, file in fileList:\n",
    "    # file.event_id = {'thumb':1, 'index':2, 'middle':3, 'ring':4, 'pinkie':5}\n",
    "    # conditions = ['thumb']\n",
    "    labels = pd.Series()\n",
    "    features = pd.DataFrame(columns=[j for i in range(19) for j in sample_freqs[0:int(t/2)]])\n",
    "    for trial_i in range(file.events.shape[0]): #for each event\n",
    "        labels.loc[len(labels)] = file.events[trial_i][2]\n",
    "        features_flat = []\n",
    "        for channel_i in range(19): #srry hardcoding this cuz lazy\n",
    "            power = get_fft(file, trial_i, channel_i)\n",
    "            features_flat.extend(power)\n",
    "        features.loc[len(features)] = features_flat\n",
    "    # features.to_csv(f'{subj}/fft/features.csv', index=False)\n",
    "    # labels.to_csv(f'{subj}/fft/labels.csv', index=False)\n",
    "    all_features[subj] = features\n",
    "    all_labels[subj] = labels\n",
    "\n",
    "#   print(f'{subj}: {file.events.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy subj_A: 0.2718978102189781\n",
      "Accuracy subj_B: 0.4291958041958042\n",
      "Accuracy subj_C: 0.4807017543859649\n",
      "Accuracy subj_E: 0.49005847953216375\n",
      "Accuracy subj_F: 0.3267670915411356\n",
      "Accuracy subj_G: 0.2714535901926445\n",
      "Accuracy subj_H: 0.30324909747292417\n",
      "Accuracy subj_I: 0.3524305555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# SVM TIME WOOO\n",
    "for subj, file in fileList:\n",
    "    features = all_features[subj]\n",
    "    labels = all_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    \n",
    "    classifier = svm.SVC(kernel='rbf') # radial basis kernel; gamma should be 0.1\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: try morlet??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfr.data.shape)\n",
    "freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequencies of interest\n",
    "freqs = np.logspace(*np.log10([6, 30]), num=20) \n",
    "\n",
    "for subj, file in fileList:\n",
    "    tfr = mne.time_frequency.tfr_morlet(file, freqs=freqs, n_cycles=freqs / 2., return_itc=False)\n",
    "    features = tfr.data.reshape((tfr.data.shape[0]*tfr.data.shape[1]* tfr.data.shape[2]))\n",
    "    labels = [file.events[trial][2] for trial in file.events.shape[0]]  # Extract labels from the epochs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
