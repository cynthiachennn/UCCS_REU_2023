{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# some hyperparameter type things :)\n",
    "sr = 200\n",
    "t = 200\n",
    "ch = 19\n",
    "affix = '_clean_noref'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: obtain epochs and evokeds representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "input_fname does not exist: /net/home/store/home/cchen4/5F_EEG_DATA/preprocessing/A_405_clean_noref.set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m subjList \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mA_405\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mA_408\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_110\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_309\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_311\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_316\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC_204\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC_429\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE_321\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE_415\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE_429\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF_027\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF_209\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF_210\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG_413\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG_428\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mH_804\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI_719\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI_723\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# exclude 'G_413' because it only has 3 events? and the other G only has like 30 events so like wtf :(  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m fileNameList \u001b[39m=\u001b[39m [(subj, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpreprocessing/\u001b[39m\u001b[39m{\u001b[39;00msubj\u001b[39m}\u001b[39;00m\u001b[39m_clean_noref.set\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m subj \u001b[39min\u001b[39;00m subjList]\n\u001b[0;32m----> 4\u001b[0m fileList \u001b[39m=\u001b[39m [(subj, mne\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_raw_eeglab(fileName, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)) \u001b[39mfor\u001b[39;00m subj, fileName \u001b[39min\u001b[39;00m fileNameList]\n\u001b[1;32m      5\u001b[0m fileList \u001b[39m=\u001b[39m [(subj, file, (mne\u001b[39m.\u001b[39mevents_from_annotations(file, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))) \u001b[39mfor\u001b[39;00m subj, file \u001b[39min\u001b[39;00m fileList]\n\u001b[1;32m      6\u001b[0m fileList \u001b[39m=\u001b[39m [(subj, mne\u001b[39m.\u001b[39mEpochs(file, events[\u001b[39m0\u001b[39m], event_id\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m],tmin\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, tmax\u001b[39m=\u001b[39mt\u001b[39m/\u001b[39msr, baseline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)) \u001b[39mfor\u001b[39;00m subj, file, events \u001b[39min\u001b[39;00m fileList]\n",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m subjList \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mA_405\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mA_408\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_110\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_309\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_311\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB_316\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC_204\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC_429\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE_321\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE_415\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mE_429\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF_027\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF_209\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF_210\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG_413\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG_428\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mH_804\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI_719\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI_723\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# exclude 'G_413' because it only has 3 events? and the other G only has like 30 events so like wtf :(  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m fileNameList \u001b[39m=\u001b[39m [(subj, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpreprocessing/\u001b[39m\u001b[39m{\u001b[39;00msubj\u001b[39m}\u001b[39;00m\u001b[39m_clean_noref.set\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m subj \u001b[39min\u001b[39;00m subjList]\n\u001b[0;32m----> 4\u001b[0m fileList \u001b[39m=\u001b[39m [(subj, mne\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mread_raw_eeglab(fileName, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)) \u001b[39mfor\u001b[39;00m subj, fileName \u001b[39min\u001b[39;00m fileNameList]\n\u001b[1;32m      5\u001b[0m fileList \u001b[39m=\u001b[39m [(subj, file, (mne\u001b[39m.\u001b[39mevents_from_annotations(file, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))) \u001b[39mfor\u001b[39;00m subj, file \u001b[39min\u001b[39;00m fileList]\n\u001b[1;32m      6\u001b[0m fileList \u001b[39m=\u001b[39m [(subj, mne\u001b[39m.\u001b[39mEpochs(file, events[\u001b[39m0\u001b[39m], event_id\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m],tmin\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, tmax\u001b[39m=\u001b[39mt\u001b[39m/\u001b[39msr, baseline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)) \u001b[39mfor\u001b[39;00m subj, file, events \u001b[39min\u001b[39;00m fileList]\n",
      "File \u001b[0;32m/net/home/store/home/cchen4/5F_EEG_DATA/.venv/lib/python3.10/site-packages/mne/io/eeglab/eeglab.py:311\u001b[0m, in \u001b[0;36mread_raw_eeglab\u001b[0;34m(input_fname, eog, preload, uint16_codec, montage_units, verbose)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m@fill_doc\u001b[39m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_raw_eeglab\u001b[39m(\n\u001b[1;32m    272\u001b[0m     input_fname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    278\u001b[0m ):\n\u001b[1;32m    279\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Read an EEGLAB .set file.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m    .. versionadded:: 0.11.0\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mreturn\u001b[39;00m RawEEGLAB(\n\u001b[1;32m    312\u001b[0m         input_fname\u001b[39m=\u001b[39;49minput_fname,\n\u001b[1;32m    313\u001b[0m         preload\u001b[39m=\u001b[39;49mpreload,\n\u001b[1;32m    314\u001b[0m         eog\u001b[39m=\u001b[39;49meog,\n\u001b[1;32m    315\u001b[0m         uint16_codec\u001b[39m=\u001b[39;49muint16_codec,\n\u001b[1;32m    316\u001b[0m         montage_units\u001b[39m=\u001b[39;49mmontage_units,\n\u001b[1;32m    317\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    318\u001b[0m     )\n",
      "File \u001b[0;32m<decorator-gen-305>:10\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, input_fname, eog, preload, uint16_codec, montage_units, verbose)\u001b[0m\n",
      "File \u001b[0;32m/net/home/store/home/cchen4/5F_EEG_DATA/.venv/lib/python3.10/site-packages/mne/io/eeglab/eeglab.py:429\u001b[0m, in \u001b[0;36mRawEEGLAB.__init__\u001b[0;34m(self, input_fname, eog, preload, uint16_codec, montage_units, verbose)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39m@verbose\u001b[39m\n\u001b[1;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    420\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    427\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    428\u001b[0m ):  \u001b[39m# noqa: D102\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     input_fname \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(_check_fname(input_fname, \u001b[39m\"\u001b[39;49m\u001b[39mread\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39minput_fname\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    430\u001b[0m     eeg \u001b[39m=\u001b[39m _check_load_mat(input_fname, uint16_codec)\n\u001b[1;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m eeg\u001b[39m.\u001b[39mtrials \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m<decorator-gen-0>:12\u001b[0m, in \u001b[0;36m_check_fname\u001b[0;34m(fname, overwrite, must_exist, name, need_dir, verbose)\u001b[0m\n",
      "File \u001b[0;32m/net/home/store/home/cchen4/5F_EEG_DATA/.venv/lib/python3.10/site-packages/mne/utils/check.py:250\u001b[0m, in \u001b[0;36m_check_fname\u001b[0;34m(fname, overwrite, must_exist, name, need_dir, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mPermissionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not have read permissions: \u001b[39m\u001b[39m{\u001b[39;00mfname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[39melif\u001b[39;00m must_exist:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not exist: \u001b[39m\u001b[39m{\u001b[39;00mfname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m fname\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: input_fname does not exist: /net/home/store/home/cchen4/5F_EEG_DATA/preprocessing/A_405_clean_noref.set"
     ]
    }
   ],
   "source": [
    "# read info from all files.\n",
    "subjList = ['A_405', 'A_408', 'B_110', 'B_309', 'B_311', 'B_316', 'C_204', 'C_429', 'E_321', 'E_415', 'E_429', 'F_027', 'F_209', 'F_210', 'G_413', 'G_428', 'H_804', 'I_719', 'I_723'] # exclude 'G_413' because it only has 3 events? and the other G only has like 30 events so like wtf :(  \n",
    "fileNameList = [(subj, f'preprocessing/{subj}_clean_noref.set') for subj in subjList]\n",
    "fileList = [(subj, mne.io.read_raw_eeglab(fileName, verbose=False)) for subj, fileName in fileNameList]\n",
    "fileList = [(subj, file, (mne.events_from_annotations(file, verbose=False))) for subj, file in fileList]\n",
    "fileList = [(subj, mne.Epochs(file, events[0], event_id=[1, 2, 3, 4, 5],tmin= 0, tmax=t/sr, baseline=None, preload=True, verbose=False)) for subj, file, events in fileList]\n",
    "\n",
    "fileListDict = dict(fileList)\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: # \n",
    "    r = re.compile(f'{letter}_d*')\n",
    "    subjects = list(filter(r.match, subjList))\n",
    "    subjects = [fileListDict[x] for x in subjects]\n",
    "    fileName = f'subj_{letter}'\n",
    "    file = mne.concatenate_epochs(subjects)\n",
    "    fileList.append((f'subj_{letter}', file, file.events[:, -1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evokeds = {subject[1][c].average() for c in ['1', '2', '3', '4', '5']}\n",
    "for c in evokeds:\n",
    "    c.plot_joint(title=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, file, labels in fileList:\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data as features/labels\n",
    "all_features = {}\n",
    "all_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    features = pd.DataFrame(columns=[j for i in range(19) for j in range(401)])\n",
    "    for trial_i in range(file.events.shape[0]):\n",
    "        features.loc[len(features)] = file.get_data(item=trial_i).reshape(19*401) # :p i loveee hardcoding :D\n",
    "    all_features[subj] = features\n",
    "    all_labels[subj] = pd.Series(labels)\n",
    "\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    all_features[subj].to_pickle(f'pickles/{subj}_features_clean_noref.pkl')\n",
    "    all_labels[subj].to_pickle(f'pickles/{subj}_labels_clean_noref.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from .pkl files (\"raw\")\n",
    "all_features = {}\n",
    "all_labels = {}\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: \n",
    "    all_features[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_features_raw.npy')\n",
    "    all_labels[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_labels_raw.npy')\n",
    "    fileList.append((f'subj_{letter}', all_features[f'subj_{letter}'], all_labels[f'subj_{letter}']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from .pkl files (pseudosample)\n",
    "# THESE WERE NOT FIXED YET FOR LABELS -1\n",
    "pseudosampled_features = {}\n",
    "pseudosampled_labels = {}\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: \n",
    "    pseudosampled_features[f'subj_{letter}'] = pd.read_pickle(f'subj_{letter}/data/pseudosampled_features_clean.pkl')\n",
    "    pseudosampled_labels[f'subj_{letter}'] = pd.read_pickle(f'subj_{letter}/data/pseudosampled_labels_clean.pkl')\n",
    "    fileList.append((f'subj_{letter}', all_features[f'subj_{letter}'], all_labels[f'subj_{letter}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudosampling\n",
    "\n",
    "window = 100 # window size\n",
    "strides = [75, 50, 25, 25, 50, 75, 0] # step size (but actually it's better to make it BELL CURVE OMGGGGGGG (for overlap % maybe then...))\n",
    "pseudosampled_features = {}\n",
    "pseudosampled_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    features = pd.DataFrame(columns=[j for i in range(19) for j in range(100)])\n",
    "    for trial_i in range(file.events.shape[0]):\n",
    "        start = 0\n",
    "        for s in strides:\n",
    "            end = start + window\n",
    "            features.loc[len(features)] = file.get_data(item=trial_i)[0].T[start:end].reshape(19*window)\n",
    "            start += s\n",
    "    pseudosampled_features[subj] = features\n",
    "    pseudosampled_labels[subj] = labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: FFT Time !! slay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do fft for everyone :)\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_freqs = fftpack.fftfreq(t) # since all samples are the same, sample freqs are the same\n",
    "\n",
    "fft_features = {}\n",
    "fft_labels = {}\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    # file.event_id = {'thumb':1, 'index':2, 'middle':3, 'ring':4, 'pinkie':5}\n",
    "    # conditions = ['thumb']\n",
    "    fft_features[subj] = np.zeros((labels.shape[0], ch, int(t/2)))\n",
    "    for trial_i in range(labels.shape[0]): #for each event\n",
    "        fft = np.zeros((ch, int(t/2)))\n",
    "        for channel_i in range(ch):\n",
    "            sample = features[trial_i][channel_i]\n",
    "            sig_fft = fftpack.fft(sample)\n",
    "            power = np.abs(sig_fft)\n",
    "            sample_freq = fftpack.fftfreq(sample.size)\n",
    "            fft[channel_i] = power[0:int(power.size/2)]\n",
    "        fft_features[subj][trial_i] = fft\n",
    "    fft_labels[subj] = labels\n",
    "    features = fft_features[subj]\n",
    "#   print(f'{subj}: {file.events.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_stft(file, trial, channel):\n",
    "    #sample = file.get_data(item=trial, picks=channel)[0][0]\n",
    "    sample = file.loc[trial][channel*400:(channel+1)*400]\n",
    "    f, t, Zxx = stft(sample, fs=200, window='triang', nperseg=100, return_onesided=True, boundary= None)\n",
    "    Zxx = np.abs(Zxx)\n",
    "    print(Zxx.shape)\n",
    "    # plt.pcolormesh([t], f, np.abs(Zxx), vmin=Zxx.min(), vmax=Zxx.max(), shading='gourand')\n",
    "    # plt.title('STFT Magnitude')\n",
    "    # plt.ylabel('Frequency [Hz]')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.show()\n",
    "    return f, t, Zxx\n",
    "\n",
    "file = all_features['subj_A']\n",
    "STFT = get_stft(file, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = fileList[0:1]\n",
    "STFT_features = {}\n",
    "STFT_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    features = pd.DataFrame(columns=[j for j in range(387) for i in range(19)])\n",
    "    for trial_i in range(labels.shape[0]): #for each event\n",
    "        features_flat = []\n",
    "        for channel_i in range(19): #srry hardcoding this cuz lazy\n",
    "            sample = file.loc[trial_i][channel_i*400:(channel_i+1)*400]\n",
    "            f, t, Zxx = stft(sample, fs=200, window='triang', boundary= None)\n",
    "            features_flat.extend(np.reshape(Zxx, (Zxx.shape[0]*Zxx.shape[1])))\n",
    "        features.loc[len(features)] = features_flat\n",
    "    # features.to_csv(f'{subj}/fft/features.csv', index=False)\n",
    "    # labels.to_csv(f'{subj}/fft/labels.csv', index=False)\n",
    "    STFT_features[subj] = features\n",
    "    STFT_labels[subj] = labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ConvNet_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet_1D, self).__init__()\n",
    "        ### FILL IN ### [10 POINTS]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(19, 32, 3),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(121728, 256),\n",
    "            nn.Linear(6080, 256), # all features norefer\n",
    "            # nn.Linear(1408, 256), #pseudosampled featrues .\n",
    "            nn.Linear(256, 5)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STFT_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m subj, features, labels \u001b[39min\u001b[39;00m fileList:\n\u001b[0;32m----> 9\u001b[0m     features \u001b[39m=\u001b[39m STFT_features[subj]\n\u001b[1;32m     10\u001b[0m     labels \u001b[39m=\u001b[39m STFT_labels[subj]\n\u001b[1;32m     11\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(features, labels, test_size\u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m) \u001b[39m# 70% training 30% test\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STFT_features' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = STFT_features[subj]\n",
    "    labels = STFT_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    conv_net = ConvNet_1D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    for epoch in range(10):\n",
    "        print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = X_train.to_numpy()\n",
    "        targets = y_train.to_numpy()\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 19, int(data.shape[1]/19)), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "\n",
    "        # Evaluate the neural network\n",
    "        conv_net.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            data = X_test.to_numpy()\n",
    "            targets = y_test.to_numpy()\n",
    "            data = torch.tensor(data.reshape(int(data.shape[0]), 19, int(data.shape[1]/19)), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "            data = data.to(device)\n",
    "            targets = torch.tensor(targets, dtype=torch.int64)\n",
    "            targets = targets.to(device)\n",
    "            conv_net_predictions = conv_net(data)\n",
    "            _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        print(\"test accuracy = \",correct/total)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eeg(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idc):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ConvNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet_2D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, (7, 3)),\n",
    "            nn.ReLU(), \n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, (5, 3)),\n",
    "            nn.Conv2d(16, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(121728, 256),\n",
    "            nn.Linear(21056, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4), # all features norefer\n",
    "            # nn.Linear(1408, 256), #pseudosampled featrues .\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary class ?:D\n",
    "for subj, features, labels in fileList[0:1]:\n",
    "    joint = features.assign(label=labels.values)\n",
    "    joint =  joint.loc[joint['label'].isin([0, 4])]\n",
    "    features = joint.drop(['label'], axis=1)\n",
    "    labels = joint['label']\n",
    "\n",
    "    all_features[subj] = features\n",
    "    all_labels[subj] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy subj_A=  0.9803082191780822\n",
      "test accuracy subj_A=  0.31420765027322406 ****\n",
      "train accuracy subj_B=  0.9663658736669402\n",
      "test accuracy subj_B=  0.25688073394495414 ****\n",
      "train accuracy subj_C=  0.9728395061728395\n",
      "test accuracy subj_C=  0.3473684210526316 ****\n",
      "train accuracy subj_E=  0.9237102085620198\n",
      "test accuracy subj_E=  0.2578947368421053 ****\n",
      "train accuracy subj_F=  0.9249592169657422\n",
      "test accuracy subj_F=  0.21391304347826087 ****\n",
      "train accuracy subj_G=  0.9819078947368421\n",
      "test accuracy subj_G=  0.2677165354330709 ****\n",
      "train accuracy subj_H=  0.969439728353141\n",
      "test accuracy subj_H=  0.25405405405405407 ****\n",
      "train accuracy subj_I=  0.9608482871125612\n",
      "test accuracy subj_I=  0.2916666666666667 ****\n",
      "avg accuracy = 0.27546273021812095\n"
     ]
    }
   ],
   "source": [
    "# run CNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "torch.cuda.set_device(3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# device = 'cpu'\n",
    "accuracies = []\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = fft_features[subj]\n",
    "    labels = fft_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.2, random_state=42) # 70% training 30% test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= 0.2, random_state=42) # 70% training 30% test\n",
    "    conv_net = ConvNet_2D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    train_acc = 0\n",
    "    for epoch in range(50):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = X_train\n",
    "        targets = y_train\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        total, correct = 0, 0\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "        train_acc = correct/total\n",
    "\n",
    "    print(f\"train accuracy {subj}= \", train_acc) \n",
    "\n",
    "    # Evaluate the neural network\n",
    "    conv_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = X_test\n",
    "        targets = y_test\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj}= \",correct/total, '****')\n",
    "    accuracies.append(correct/total)\n",
    "\n",
    "print(f'avg accuracy = {np.mean(accuracies)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN w/ channels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 19, 200) (1826,)\n",
      "(1826, 19, 100) (1826, 19, 100)\n"
     ]
    }
   ],
   "source": [
    "print(fileList[0][1].shape, fileList[0][2].shape)\n",
    "print(fft_features['subj_A'].shape, fft_features['subj_A'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 1900)\n",
      "Accuracy subj_A: 0.2937956204379562\n",
      "(3811, 1900)\n",
      "Accuracy subj_B: 0.3723776223776224\n",
      "(1899, 1900)\n",
      "Accuracy subj_C: 0.41578947368421054\n",
      "(2848, 1900)\n",
      "Accuracy subj_E: 0.3157894736842105\n",
      "(2874, 1900)\n",
      "Accuracy subj_F: 0.25028968713789107\n",
      "(1901, 1900)\n",
      "Accuracy subj_G: 0.31523642732049034\n",
      "(922, 1900)\n",
      "Accuracy subj_H: 0.21299638989169675\n",
      "(1917, 1900)\n",
      "Accuracy subj_I: 0.2881944444444444\n"
     ]
    }
   ],
   "source": [
    "# SVM TIME WOOO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = fft_features[subj].reshape(fft_features[subj].shape[0], -1)\n",
    "    # print(features.shape)\n",
    "    labels = fft_labels[subj]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    \n",
    "    classifier = svm.SVC(kernel='rbf') # radial basis kernel; gamma should be 0.1\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TIME WOOO\n",
    "# SVM w/ 2 seconds of data takes much more time and does not improve accuracy :(\n",
    "# w/ 0.85 seconds it works .2 % maybe better and takes 1/3 of the time\n",
    "\n",
    "avg_acc = 0\n",
    "for subj, features, labels in fileList:\n",
    "    features = STFT_features[subj]\n",
    "    labels = STFT_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    classifier = svm.SVC(kernel='rbf') # radial basis kernel; gamma should be 0.1\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    avg_acc += metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Average accuracy: \", avg_acc/len(fileList))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest ? !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for subj, file in fileList:\n",
    "    features = pseudosampled_features[subj]\n",
    "    labels = pseudosampled_labels[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: try morlet??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfr.data.shape)\n",
    "freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequencies of interest\n",
    "freqs = np.logspace(*np.log10([6, 30]), num=20) \n",
    "\n",
    "for subj, file in fileList:\n",
    "    tfr = mne.time_frequency.tfr_morlet(file, freqs=freqs, n_cycles=freqs / 2., return_itc=False)\n",
    "    features = tfr.data.reshape((tfr.data.shape[0]*tfr.data.shape[1]* tfr.data.shape[2]))\n",
    "    labels = [file.events[trial][2] for trial in file.events.shape[0]]  # Extract labels from the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CSP features :D\n",
    "# use raw data frames not mne.epochs datastructure\n",
    "\n",
    "from mne.decoding import CSP\n",
    "\n",
    "csp_features = {}\n",
    "csp_labels = {}\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    csp = CSP(n_components=5, reg=None, log=True)\n",
    "    data = np.reshape(all_features[subj], (all_features[subj].shape[0], 19, 401))\n",
    "    labels = all_labels[subj]\n",
    "\n",
    "    csp_features[subj] = csp.fit_transform(data, labels)\n",
    "    csp_labels[subj] = labels\n",
    "\n",
    "    csp.plot_filters(file.info, title='CSP patterns for %s' % subj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
