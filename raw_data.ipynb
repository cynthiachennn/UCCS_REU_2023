{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the raw data :')\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import re\n",
    "import random\n",
    "\n",
    "# some hyperparameter type things :)\n",
    "sr = 200\n",
    "t = 200\n",
    "ch = 19\n",
    "affix = '_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rawdata(subj):\n",
    "    sr = 1000 if subj[1] == '_HFREQ' else 200\n",
    "    mat = scipy.io.loadmat(f'preprocessing/raw_data/{subj[0]}{subj[1]}.mat')\n",
    "    data = mat['o'][0][0][5].T\n",
    "    data = np.delete(data, [10, 11, 21], axis = 0)\n",
    "\n",
    "    if sr == 1000: data = scipy.signal.resample(data, int(data.shape[1]/5), axis = 1)\n",
    "\n",
    "    events = pd.read_csv(f\"preprocessing/{subj[0]}_events.txt\", sep=\"\\t\").drop('number', axis=1)\n",
    "    events = events[['latency', 'urevent', 'type']].to_numpy()\n",
    "    events = mne.pick_events(events, include=[1, 2, 3, 4, 5]) # should replace this with a numpy function lol\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for event in events:\n",
    "        time = int(event[0])\n",
    "        if sr == 1000: time = int(time/5)\n",
    "        type = int(event[2])\n",
    "        features.append([data[i][time:time+200] for i in range(ch)])\n",
    "        labels.append(type - 1)\n",
    "\n",
    "    # mne.filter.filter_data(features, 200, 5, None)\n",
    "\n",
    "    return (np.array(features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_features = {}\n",
    "raw_labels = {}\n",
    "\n",
    "for subj in [('A_405', ''), ('A_408', '_HFREQ'), ('B_110', ''), ('B_309', '_HFREQ'), ('B_311', '_HFREQ'), ('B_316', ''), ('C_204', ''), ('C_429', '_HFREQ'), ('E_321', '_HFREQ'), ('E_415', '_HFREQ'), ('E_429', '_HFREQ'), ('F_027', ''), ('F_209', ''), ('F_210', '_HFREQ'), ('G_413', '_HFREQ'), ('G_428', '_HFREQ'), ('H_804', '_HFREQ'), ('I_719', '_HFREQ'), ('I_723', '_HFREQ')]:\n",
    "    print(subj[0])\n",
    "    raw_features[subj[0]], raw_labels[subj[0]] = load_rawdata(subj)\n",
    "\n",
    "subjList = ['A_405', 'A_408', 'B_110', 'B_309', 'B_311', 'B_316', 'C_204', 'C_429', 'E_321', 'E_415', 'E_429', 'F_027', 'F_209', 'F_210', 'G_413', 'G_428', 'H_804', 'I_719', 'I_723']\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']:\n",
    "    r = re.compile(f'{letter}_d*')\n",
    "    subjects = list(filter(r.match, subjList))\n",
    "    print(subjects)\n",
    "    subjects = [(raw_features[x], raw_labels[x]) for x in subjects]\n",
    "    fileName = f'subj_{letter}'\n",
    "    _features = np.concatenate([features for features, labels in subjects])\n",
    "    _labels = np.concatenate([labels for features, labels in subjects])\n",
    "\n",
    "    _features = np.array(_features).reshape(-1, 19, 200)\n",
    "    _labels = np.array(_labels).reshape(-1)\n",
    "    fileList.append((fileName, _features, _labels))\n",
    "\n",
    "for subj, file, labels in fileList:\n",
    "    np.save(f'pickles/{subj}_features{affix}', file)\n",
    "    np.save(f'pickles/{subj}_labels{affix}', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from .pkl files (\"raw\")\n",
    "raw_features = {}\n",
    "raw_labels = {}\n",
    "fileList = []\n",
    "\n",
    "for letter in ['A', 'B', 'C', 'E', 'F', 'G', 'H', 'I']: \n",
    "    raw_features[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_features{affix}.npy')\n",
    "    raw_labels[f'subj_{letter}'] = np.load(f'pickles/subj_{letter}_labels{affix}.npy').reshape(-1)\n",
    "    fileList.append((f'subj_{letter}', raw_features[f'subj_{letter}'], raw_labels[f'subj_{letter}']))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features extraction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_fft(sample): # pull one piece of that data set, trial 1 of thumb data\n",
    "    #fig1 = plt.plot(np.linspace(0, 1.5, num=300), sample)\n",
    "    # fft on that one piece...\n",
    "    sig_fft = fftpack.fft(sample)\n",
    "    power = np.abs(sig_fft)**2\n",
    "    sample_freq = fftpack.fftfreq(sample.size)\n",
    "    return power[0:int(power.size/2)]\n",
    "    # fig2 = plt.plot(sample_freq[0:200], power[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826, 19, 100)\n",
      "(3811, 19, 100)\n",
      "(1899, 19, 100)\n",
      "(2848, 19, 100)\n",
      "(2874, 19, 100)\n",
      "(1901, 19, 100)\n",
      "(922, 19, 100)\n",
      "(1917, 19, 100)\n"
     ]
    }
   ],
   "source": [
    "# do fft for everyone :)\n",
    "\n",
    "sample_freqs = fftpack.fftfreq(t) # since all samples are the same, sample freqs are the same\n",
    "\n",
    "fft_features = {}\n",
    "fft_labels = {}\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    # file.event_id = {'thumb':1, 'index':2, 'middle':3, 'ring':4, 'pinkie':5}\n",
    "    # conditions = ['thumb']\n",
    "    fft_features[subj] = np.zeros((labels.shape[0], ch, int(t/2)))\n",
    "    for trial_i in range(labels.shape[0]): #for each event\n",
    "        fft = np.zeros((ch, int(t/2)))\n",
    "        for channel_i in range(ch):\n",
    "            power = get_fft(features[trial_i][channel_i])\n",
    "            fft[channel_i] = power\n",
    "        fft_features[subj][trial_i] = fft\n",
    "    fft_labels[subj] = labels\n",
    "    features = fft_features[subj]\n",
    "#   print(f'{subj}: {file.events.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get CSP features :D\n",
    "from mne.decoding import CSP\n",
    "\n",
    "file = mne.io.read_raw_eeglab('preprocessing/A_405_clean.set', verbose=False)\n",
    "\n",
    "csp_features = {}\n",
    "csp_labels = {}\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    csp = CSP(n_components=5, reg=None, log=True)\n",
    "    labels = raw_labels[subj]\n",
    "\n",
    "    csp_features[subj] = csp.fit_transform(features, labels)\n",
    "    csp_labels[subj] = labels\n",
    "\n",
    "    csp.plot_filters(file.info, title='CSP patterns for %s' % subj)    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D CNN w/ Adham's recommendations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ConvNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet_2D, self).__init__()\n",
    "        ### FILL IN ### [10 POINTS]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, (7, 3)),\n",
    "            nn.ReLU(), \n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, (5, 3)),\n",
    "            nn.Conv2d(16, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(121728, 256),\n",
    "            nn.Linear(21056, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4), # all features norefer\n",
    "            # nn.Linear(1408, 256), #pseudosampled featrues .\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        x = self.hidden(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy subj_A=  0.19708029197080293\n",
      "test accuracy subj_B=  0.24737762237762237\n",
      "test accuracy subj_C=  0.21228070175438596\n",
      "test accuracy subj_E=  0.2\n",
      "test accuracy subj_F=  0.20278099652375434\n",
      "test accuracy subj_G=  0.19264448336252188\n",
      "test accuracy subj_H=  0.22743682310469315\n",
      "test accuracy subj_I=  0.21354166666666666\n",
      "avg accuracy = 0.2116428232200559\n"
     ]
    }
   ],
   "source": [
    "# run CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "accuracies = []\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    features = fft_features[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    conv_net = ConvNet_2D().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    for epoch in range(50):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = X_train\n",
    "        targets = y_train\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "\n",
    "    # Evaluate the neural network\n",
    "    conv_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = X_test\n",
    "        targets = y_test\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj}= \",correct/total)\n",
    "    accuracies.append(correct/total)\n",
    "\n",
    "print(f'avg accuracy = {np.mean(accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([0.395985401459854, 0.47465034965034963, 0.5421052631578948, 0.5941520467836258,\n",
    "  0.42526071842410196, 0.4868651488616462, 0.3285198555956679, 0.5017361111111112])/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN transfer learning version\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "conv_net = ConvNet_2D().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "conv_net_optimizer = torch.optim.Adam(conv_net.parameters(), lr=0.001)\n",
    "\n",
    "random.shuffle(fileList)\n",
    "\n",
    "for subj, features, labels in fileList[0:-1]:\n",
    "    # Train the neural network\n",
    "    for epoch in range(30):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        conv_net.train()\n",
    "        data = features\n",
    "        targets = labels\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        conv_net_loss = criterion(conv_net_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        conv_net_optimizer.zero_grad()\n",
    "        conv_net_loss.backward()\n",
    "        conv_net_optimizer.step()\n",
    "\n",
    "    # Evaluate the neural network\n",
    "    conv_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = fileList[-1][1]\n",
    "        targets = fileList[-1][2]\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "        conv_net_predictions = conv_net(data)\n",
    "        _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj} = \",correct/total)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = fileList[-1][1]\n",
    "    targets = fileList[-1][2]\n",
    "    data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "    data = data.to(device)\n",
    "    targets = torch.tensor(targets, dtype=torch.int64)\n",
    "    targets = targets.to(device)\n",
    "    conv_net_predictions = conv_net(data)\n",
    "    _, predicted = torch.max(conv_net_predictions.data, 1)\n",
    "    total += targets.size(0)\n",
    "    correct += (predicted == targets).sum().item()\n",
    "print(\"test accuracy = \",correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TIME WOOO\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "avg_acc = 0\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    #features = csp_features[subj]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.reshape(features, (features.shape[0], -1)), labels, test_size= 0.3) # 70% training 30% test\n",
    "    \n",
    "    classifier = BaggingClassifier(svm.SVC(kernel='rbf'), n_estimators=20) # radial basis kernel; er i think they used gamma=0.2 ngl\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    avg_acc += metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Average accuracy: \", avg_acc/len(fileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest ?\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    print(f\"Accuracy {subj}:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + LSTM :D\n",
    "#2D CNN w/ Adham's recommendations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Conv_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_LSTM, self).__init__()\n",
    "        ### FILL IN ### [10 POINTS]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, (5, 5)),\n",
    "            nn.Conv2d(8, 16, (3, 3)),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, (6, 1)),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.Sequential(\n",
    "            nn.LSTM(32, 512, batch_first=True, num_layers=2, dropout=0.2)\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### FILL IN ### [5 POINTS]\n",
    "        x = self.conv1(x)\n",
    "        # x.shape = (batch_size//# trials, channels, height, width)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[3])\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.lstm(x)\n",
    "        x = x[0][:, -1, :]\n",
    "        x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy subj_A=  0.3010948905109489\n",
      "test accuracy subj_B=  0.3129370629370629\n",
      "test accuracy subj_C=  0.2754385964912281\n",
      "test accuracy subj_E=  0.3929824561403509\n",
      "test accuracy subj_F=  0.2688296639629201\n",
      "test accuracy subj_G=  0.38003502626970226\n",
      "test accuracy subj_H=  0.3140794223826715\n",
      "test accuracy subj_I=  0.3802083333333333\n",
      "avg accuracy = 0.3282006815035272\n"
     ]
    }
   ],
   "source": [
    "# run CNN+LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "accuracies = []\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    conv_lstm = Conv_LSTM().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conv_lstm_optimizer = torch.optim.Adam(conv_lstm.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    for epoch in range(50):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        conv_lstm.train()\n",
    "        data = X_train\n",
    "        targets = y_train\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64) # nn.functional.one_hot().type(torch.float32)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        conv_lstm_predictions = conv_lstm(data)\n",
    "        conv_lstm_loss = criterion(conv_lstm_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        conv_lstm_optimizer.zero_grad()\n",
    "        conv_lstm_loss.backward()\n",
    "        conv_lstm_optimizer.step()\n",
    "\n",
    "    # Evaluate the neural network\n",
    "    conv_lstm.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = X_test\n",
    "        targets = y_test\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), 1, data.shape[1], data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64) # nn.functional.one_hot().type(torch.float32)\n",
    "        targets = targets.to(device)\n",
    "        conv_lstm_predictions = conv_lstm(data)\n",
    "        _, predicted = torch.max(conv_lstm_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj}= \",correct/total)\n",
    "    accuracies.append(correct/total)\n",
    "\n",
    "print(f'avg accuracy = {np.mean(accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test accuracy subj_A=  0.36313868613138683\n",
    "test accuracy subj_F=  0.593279258400927\n",
    "test accuracy subj_B=  0.15297202797202797\n",
    "test accuracy subj_G=  0.12084063047285463\n",
    "test accuracy subj_C=  0.03859649122807018\n",
    "test accuracy subj_I=  0.3263888888888889\n",
    "test accuracy subj_H=  0.10469314079422383\n",
    "test accuracy subj_E=  0.21403508771929824\n",
    "avg accuracy = 0.23924302645095968\n",
    "\n",
    "waht the heck so mixed\n",
    "\n",
    "\n",
    "another run:\n",
    "\n",
    "test accuracy subj_A=  0.09124087591240876\n",
    "test accuracy subj_F=  0.1332560834298957\n",
    "test accuracy subj_B=  0.0026223776223776225\n",
    "test accuracy subj_G=  0.09281961471103327\n",
    "test accuracy subj_C=  0.10175438596491228\n",
    "test accuracy subj_I=  0.15625\n",
    "test accuracy subj_H=  0.06859205776173286\n",
    "test accuracy subj_E=  0.09005847953216374\n",
    "avg accuracy = 0.09207423436681553\n",
    "\n",
    "\n",
    "ok finally consistent results :\n",
    "\n",
    "test accuracy subj_A=  0.3284671532846715\n",
    "test accuracy subj_B=  0.38286713286713286\n",
    "test accuracy subj_C=  0.38070175438596493\n",
    "test accuracy subj_E=  0.43391812865497076\n",
    "test accuracy subj_F=  0.26187717265353416\n",
    "test accuracy subj_G=  0.37478108581436076\n",
    "test accuracy subj_H=  0.3140794223826715\n",
    "test accuracy subj_I=  0.4010416666666667\n",
    "avg accuracy = 0.3597166895887467"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps!\n",
    "\n",
    "- try a minimal preprocessing approach and see if it improves anything (only bandpass filter and ica ???) the AAR might have messed things up last time, but not sure\n",
    "- figure out CNN LSTM ? transformers ? GRU\n",
    "- should i attempt any feature extraction ?\n",
    "- \n",
    "- maybe read raw ECoG data and apply this classifier on it to see?\n",
    "- see what happens if i combine ECoG and EEG data .. hm\n",
    "\n",
    "\n",
    "\n",
    "SUBJECT TRANSFER:\n",
    "- try randomizing samples\n",
    "- try a retuning 20% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class gru_rnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(gru_rnn, self).__init__()\n",
    "        self.gru = nn.GRU(3800, 32, 3, dropout=0.2)\n",
    "        self.fc = nn.Linear(32, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gru(x)[0]\n",
    "        # x = self.fc(self.relu(x[0]))\n",
    "        # x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gRU !\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train neural net\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "accuracies = []\n",
    "\n",
    "for subj, features, labels in fileList:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size= 0.3) # 70% training 30% test\n",
    "    neural_net = gru_rnn().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    neural_net_optimizer = torch.optim.Adam(neural_net.parameters(), lr=0.001)\n",
    "    # Train the neural network\n",
    "    for epoch in range(50):\n",
    "        # print(\"epoch = \",epoch)\n",
    "        neural_net.train()\n",
    "        data = X_train\n",
    "        targets = y_train\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), data.shape[1]* data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        neural_net_predictions = neural_net(data)\n",
    "        neural_net_loss = criterion(neural_net_predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        neural_net_optimizer.zero_grad()\n",
    "        neural_net_loss.backward()\n",
    "        neural_net_optimizer.step()\n",
    "\n",
    "    # Evaluate the neural network\n",
    "    neural_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        data = X_test\n",
    "        targets = y_test\n",
    "        data = torch.tensor(data.reshape(int(data.shape[0]), data.shape[1] * data.shape[2]), dtype=torch.float32) # reshape # of trial, 1 channel, # of samples\n",
    "        data = data.to(device)\n",
    "        targets = torch.tensor(targets, dtype=torch.int64)\n",
    "        targets = targets.to(device)\n",
    "        neural_net_predictions = neural_net(data)\n",
    "        _, predicted = torch.max(neural_net_predictions.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    print(f\"test accuracy {subj}= \",correct/total)\n",
    "    accuracies.append(correct/total)\n",
    "\n",
    "print(f'avg accuracy = {np.mean(accuracies)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
